---
title: "Tennis Modeling: ATP Tour"
author: "Mona Ascha"
date: "`Last Edited format(Sys.time(), '%d %B %Y'`"
output: html_document
---
#Load packages
```{r}

#Data Cleaning/EDA packages
library(Hmisc)
library(tidyverse)
library(knitr)
library(dplyr)
library(lubridate)
library(broom)
library(dplyr)
library(fastDummies)
library(finalfit) 
library(corrplot)
library(data.table)

#Visualization packages
library(ggplot2)

#Modeling packages
#install.packages("drat", repos="https://cran.rstudio.com")
#drat:::addRepo("dmlc")
#install.packages("xgboost", repos="http://dmlc.ml/drat/", type = "source")
library(xgboost)
library(keras)
library(randomForest)
library(e1071)
#library(neuralnet)
library(corrr)
library(FactoMineR)
library(factoextra)

#Model metrics packages
library(pROC)
library(plotROC)
```

#Load data
```{r}
#Import our csv files created in our data prep RMD
data_ids <- read.csv("/Users/mona/Dropbox/Desktop/Tennis_Analytics/tennis_preds/data_ids.csv")
data_train <- read.csv("/Users/mona/Dropbox/Desktop/Tennis_Analytics/tennis_preds/data_train.csv")

```

#Scale/transform variables
```{r}

#We will scale using the scale variable in R
#https://stackoverflow.com/questions/49260862/trainable-sklearn-standardscaler-for-r

data_x_train_scaled <- as.data.frame(scale(data_train))
data_y_train <- as.data.frame(data_ids$Win_P_1)
colnames(data_y_train) <- "label"

#It looks like after scaling we have two columns with missing values, so we will drop them for now 
to_drop <- c("bp_ratio_av_P_1",
             "bp_ratio_av_P_2")

data_x_train_scaled <- data_x_train_scaled %>% select(!to_drop)

```

#Modeling Plan
```{r}
#The outcome variable of interest is: data_ids$Win_P_1

#We have many features, so let's do some feature selection with RF and PCA prior to running our models

#We will build out the following models
#1. Random forest feature importance + regression
#2. Random forest feature importance + SVM
#3. Random forest feature importance + XGBoost
#4. PCA + regression
#5. PCA + SVM
#6. PCA + random forest
#7. PCA + XGBoost
#8. Shallow neural network 

```

#Random forest in R for feature importance 

```{r}
#Baseline RF with the whole dataset 

set.seed(42)

data_y_rf <- as.factor(data_ids$Win_P_1)

rf <- randomForest(y=data_y_rf, 
                   x=data_x_train_scaled, 
                   proximity=TRUE,
                   replace=TRUE,
                   importance=TRUE) 

print(rf)

#Out of bag error is 12.35%, so the train data set model accuracy is around 87%.
#500 trees, 10 variables at each split

#Let's look at feature importance, i.e. the top 10 most important features
head(rf$importance, 10)

#We can see how it classifies each sample
first <- as.data.frame(rf$votes)
second <- data_y_train
head(cbind(first, second), 10)
tail(cbind(first, second), 10)

#Visualizations
plot(rf,
     main="Random Forest Error Rate",
     #xlab="Number of Trees",
     #ylab="Error Rate", 
     lwd=1.5,
     panel.first={
        axis(1, tck=1, col.ticks="light gray")
        axis(1, tck=-0.015, col.ticks="black")
        axis(2, tck=1, col.ticks="light gray", lwd.ticks="1")
        axis(2, tck=-0.015)
        minor.tick(nx=5, ny=2, tick.ratio=0.5)
        box()
 })

#Tree size
hist(treesize(rf),
     main = "No. of Nodes for the Trees",
     col = "green", 
     xlab="Tree Size",
     breaks=20, 
     xlim=c(975, 1200))

#Variable Importance
varImpPlot(rf,
           sort = T,
           n.var = 20,
           main = "Top 20 - Variable Importance")

#Let's take the top 20 important variables
top20_featimp <- rownames(head(rf$importance, 20))

```

#Data partitioning and prep for modeling
```{r}

#Bring the data back and shuffle it
set.seed(42)
all_data <- cbind(data_x_train_scaled, data_y_train)
rows <- sample(nrow(all_data))
all_data <- all_data[rows,]
  
#Data partition for our subsequent models
set.seed(101) 

#Selecting 75% of data as sample from total 'n' rows of the data  
sample <- sample.int(n = nrow(all_data), size = floor(.75*nrow(all_data)), replace = F)
train <- all_data[sample, ]
test  <- all_data[-sample, ]

#Getting it into X_train, Y_train, X_test, Y_test
X_train <- train %>% select(!label)
Y_train <- train %>% select(label)
X_test <- test %>% select(!label)
Y_test <- test %>% select(label)

#We only want our top 20 features for modeling
X_train_rf <- X_train[, top20_featimp]
X_test_rf <- X_test[, top20_featimp]

#All data
data_x_rf <- data_x_train_scaled[, top20_featimp]
#data_y_train is the corresponding label
```

#Model 1: RF with top 20 variables + logistic regression

```{r}
#LR model
rf_lr_model <- glm(unlist(Y_train) ~ ., 
                   data=X_train_rf, 
                   family="binomial")


summary(rf_lr_model)

#Confusion matrix with training preds
predicted_values <- as.data.frame(rf_lr_model$fitted.values)
predicted_classes <- ifelse(predicted_values > 0.5, 1, 0)

colnames(predicted_classes) <- "Predicted"

training_preds_actual <- cbind(Y_train, predicted_classes)
colnames(training_preds_actual) <- c("Actual", "Predicted")

Actual <- factor(c(0, 0, 1, 1))
Predicted <- factor(c(0, 1, 0, 1))
values <- table(training_preds_actual)[1:4]
cf_data <- data.frame(Actual, Predicted, values)

ggplot(cf_data=training_preds_actual, mapping = aes(x = Actual, y = Predicted)) +
  geom_tile(aes(fill = values), colour = "white") +
  geom_text(aes(label = sprintf("%1.0f", values)), vjust = 1) +
  scale_fill_gradient(low = "blue", high = "red") +
  theme_bw() + theme(legend.position = "none") + 
  ggtitle("RF + LR: Confusion Matrix")

confusion_matrix <- as.data.frame(table(training_preds_actual))

#Confusion matrix with test preds
predicted_values <- data.frame(probs = predict(rf_lr_model, newdata=X_test_rf, type="response"))
predicted_classes <- ifelse(predicted_values > 0.5, 1, 0)

colnames(predicted_classes) <- "Predicted"

test_preds_actual <- cbind(Y_test, predicted_classes)
colnames(test_preds_actual) <- c("Actual", "Predicted")

Actual <- factor(c(0, 0, 1, 1))
Predicted <- factor(c(0, 1, 0, 1))
values <- table(test_preds_actual)[1:4]
cf_data <- data.frame(Actual, Predicted, values)

ggplot(cf_data=test_preds_actual, mapping = aes(x = Actual, y = Predicted)) +
  geom_tile(aes(fill = values), colour = "white") +
  geom_text(aes(label = sprintf("%1.0f", values)), vjust = 1) +
  scale_fill_gradient(low = "blue", high = "red") +
  theme_bw() + theme(legend.position = "none") + 
  ggtitle("Test Set RF + LR: Confusion Matrix")

confusion_matrix <- as.data.frame(table(test_preds_actual))

#Test set ROC curve
rf_lr_test_prob <- predict(rf_lr_model, newdata = X_test_rf, type = "response")
rf_lr_test_roc = roc(unlist(Y_test) ~ rf_lr_test_prob, 
               auc = TRUE,
               plot = TRUE, 
               ci = TRUE, 
               ci.type = "bars",
               print.auc = TRUE,
               grid = TRUE, 
               percent = TRUE,
            print.thres.col="blue",
            print.thres.cex=0.7,
            main="Test Set RF + LR: ROC Curve")

```

#Model 2: RF with top 20 variables + SVM

```{r}
#We have X_train_rf, X_test_rf, Y_train, Y_test

#Turn y into a factor variable
Y_train_rf_svm <- as.factor(Y_train$label)
Y_test_rf_svm <- as.factor(Y_test$label)

#Create a merged dataframe of predictors and labels
data_train_rf_svm <- data.frame(X_train_rf, 
                                y = Y_train_rf_svm)

#Make a call to svm, using y as the response variable and other variables as the predictors. The dataframe will have unpacked the matrix x into 2 columns named x1 and x2. Kernel is linear, the tune-in parameter cost is 100, and scale equals false. We don't need to scale since that's already been done.

#Can try different cost function
rf_svm <- svm(y ~ ., 
              data = data_train_rf_svm, 
              kernel = "linear", 
              cost = 10, 
              scale = FALSE, 
              probability=TRUE)

print(rf_svm)

#There's a plot function for SVM that shows the decision boundary.
#In the plot, points that are represented by an “X” are the support vectors, or the points that directly affect the classification line. The points marked with an “o” are the other points, which don’t affect the calculation of the line.

#Get our test set predictions for confusion matrix and ROC curve
rf_svm_test_preds <- predict(rf_svm, X_test_rf, probability = TRUE)
rf_svm_test_preds <- as.data.frame(attr(rf_svm_test_preds, "probabilities"))
colnames(rf_svm_test_preds) <- c("Lose", "Win")
rf_svm_test_preds_win <- rf_svm_test_preds$Win

#Test Set ROC curve
rf_svm_roc <- roc(as.numeric(Y_test_rf_svm)-1,
                   rf_svm_test_preds_win,
                   auc=TRUE, 
                   ci=TRUE, 
                   ci.type="bars",
                   plot=TRUE, 
                   grid=TRUE, 
                   percent=TRUE, 
                   boot.n=100, 
                   ci.alpha=0.9, 
                   stratified=FALSE,
                   show.thres=TRUE, 
                   legacy.axes=TRUE,
                   reuse.auc=TRUE, 
                   print.auc=TRUE,
                   print.thres.col="blue",
                   print.thres.cex=0.7,
                   main="Test Set RF + Linear SVM: ROC Curve")

#Trying out a radial kernel with a cost function of 10
rf_svm_radial <- svm(factor(y) ~ ., 
              data = data_train_rf_svm, 
              kernel = "radial", 
              cost = 10, 
              scale = FALSE, 
              probability=TRUE)

print(rf_svm_radial)

table(predicted=rf_svm_radial$fitted,actual=data_train_rf_svm$y)

#~4800 support vectors...not great 

#There's a plot function for SVM that shows the decision boundary.
#In the plot, points that are represented by an “X” are the support vectors, or the points that directly affect the classification line. The points marked with an “o” are the other points, which don’t affect the calculation of the line. 
#Principal component 1 on x axis and component 2 on y axis

#Get our test set predictions for confusion matrix and ROC curve
rf_svm_radial_test_preds <- predict(rf_svm_radial, X_test_rf, probability=TRUE)
rf_svm_radial_test_preds <- as.data.frame(attr(rf_svm_radial_test_preds, "probabilities"))
colnames(rf_svm_radial_test_preds) <- c("Lose", "Win")
rf_svm_radial_test_preds_win <- rf_svm_radial_test_preds$Win

#Test Set ROC curve
rf_svm_radial_roc <- roc(as.numeric(Y_test_rf_svm)-1,
                          rf_svm_radial_test_preds_win,
                   auc=TRUE, 
                   ci=TRUE, 
                   ci.type="bars",
                   plot=TRUE, 
                   grid=TRUE, 
                   percent=TRUE, 
                   boot.n=100, 
                   ci.alpha=0.9, 
                   stratified=FALSE,
                   show.thres=TRUE, 
                   legacy.axes=TRUE,
                   reuse.auc=TRUE, 
                   print.auc=TRUE,
                   print.thres.col="blue",
                   print.thres.cex=0.7,
                   main="Test Set RF + Radial SVM: ROC Curve")

rf_svm_radial_roc

```

#Model 4: RF with top 20 variables + XGBoost 

```{r}

rf_xgboost <- xgboost(data = data.matrix(X_train_rf), 
                       label = as.numeric(Y_train$label), 
                       eta = 0.1, 
                       nrounds = 25, 
                      #booster='gbtree',
                      #max_depth = 15,
                      #subsample = 0.5,
                       objective = "binary:logistic")


#Let's predict
rf_xgb_test_pred <- predict(rf_xgboost, data.matrix(X_test_rf))
print(head(rf_xgb_test_pred))

#Convert to 0 and 1 
rf_xgb_test_pred_class <- as.numeric(rf_xgb_test_pred > 0.5)
print(head(rf_xgb_test_pred_class))

#Measure model performance
err <- mean(as.numeric(rf_xgb_test_pred > 0.5) != Y_test$label)
print(paste("test-error=", err))

#Confusion matrix 
predicted_classes <- as.data.frame(rf_xgb_test_pred_class)
colnames(predicted_classes) <- "Predicted"

test_preds_actual <- cbind(Y_test, predicted_classes)
colnames(test_preds_actual) <- c("Actual", "Predicted")

Actual <- factor(c(0, 0, 1, 1))
Predicted <- factor(c(0, 1, 0, 1))
values <- table(test_preds_actual)[1:4]
cf_data <- data.frame(Actual, Predicted, values)

ggplot(cf_data=test_preds_actual, mapping = aes(x = Actual, y = Predicted)) +
  geom_tile(aes(fill = values), colour = "white") +
  geom_text(aes(label = sprintf("%1.0f", values)), vjust = 1) +
  scale_fill_gradient(low = "blue", high = "red") +
  theme_bw() + theme(legend.position = "none") + 
  ggtitle("Test Set RF + LR: Confusion Matrix")

confusion_matrix <- as.data.frame(table(training_preds_actual))

#ROC curve
rf_xgb_roc <- roc(as.numeric(Y_test$label),
                  as.numeric(rf_xgb_test_pred),
                   auc=TRUE, 
                   ci=TRUE, 
                   ci.type="bars",
                   plot=TRUE, 
                   grid=TRUE, 
                   percent=TRUE, 
                   boot.n=100, 
                   ci.alpha=0.9, 
                   stratified=FALSE,
                   show.thres=TRUE, 
                   legacy.axes=TRUE,
                   reuse.auc=TRUE, 
                   print.auc=TRUE,
                   print.thres.col="blue",
                   print.thres.cex=0.7,
                   main="Test Set RF + XGBoost: ROC Curve")

rf_xgb_roc

```

#Pre modeling PCA

```{r}
options(ggrepel.max.overlaps = Inf)

#We have 104 variables
#We will do PCA as an alternative for dimension reduction (other method was RF feature importance)
#We will use the top 20 components (just like for RF, we use the top 20 most important features)

#We have:
##R package for correlation analysis
##Mainly used for multivariate exploratory data analysis; the factoMineR package gives access to the PCA module to perform principal component analysis. 
##factoextra provides all the relevant functions to visualize the outputs of the principal component analysis. These functions include but are not limited to scree plot, biplot, only to mention two of the visualization techniques covered later in the article.

#Create the correlation matrix
corr_matrix <- cor(X_train)

#The covariance matrix is not positive definite, as evidenced by negative eigenvalues
#This will throw an error with princomp
cov_matrix <- cov(X_train)
eigenvalues <- eigen(cov_matrix)$values
print(eigenvalues)

#One common approach to handle non-positive definite covariance matrices is to add a small value to the diagonal elements (regularization)--this can make the matrix positive definite
epsilon <- 1e-6
cov_matrix_regularized <- cov_matrix + diag(epsilon, ncol(cov_matrix))
eigenvalues_regularized <- eigen(cov_matrix_regularized)$values
print(eigenvalues_regularized)

#Now run the PCA model 
pca_data <- princomp(covmat=cov_matrix_regularized)
summary(pca_data)
plot(pca_data)

#We should still check for multicollinearity 
#Let's look for perfect correlations
perfect_correlations <- which(abs(corr_matrix) == 1, arr.ind = TRUE)
#Remove self correlations
perfect_correlations <- perfect_correlations[perfect_correlations[, 1] != perfect_correlations[, 2], ]
#See which variables are perfectly correlated
if (nrow(perfect_correlations) == 0) {
  cat("No perfect correlations found.\n")
} else {
  cat("Perfect correlations (1 or -1) found between the following variable pairs:\n")
  apply(perfect_correlations, 1, function(index_pair) {
    cat(colnames(cor_matrix)[index_pair[1]], "-", colnames(cor_matrix)[index_pair[2]], "\n")
  })
}

#Ok well this makes sense, because each variable is the inverse of each other 
# Perfect correlations (1 or -1) found between the following variable pairs:
# returnwon_perc_total_av_P_1 - servewon_perc_total_av_P_1 
# servewon_perc_total_av_P_1 - returnwon_perc_total_av_P_1 
# bp_saved_perc_av_P_2 - win_bp_perc_av_P_2 
# win_bp_perc_av_P_2 - bp_saved_perc_av_P_2 

#win_bp_perc_av_P_2           51  71
#bp_saved_perc_av_P_2         71  51
#returnwon_perc_total_av_P_1  46  44
#servewon_perc_total_av_P_1   44  46

#PCA covariance matrix is still not positive definite after dropping multicollinear variables
#We will stick with regularization technique 

#We created 104 principal components
#What are the loadings? Let's examine the first two components
pca_data$loadings[,1:2]

#Scree plot
#This plot shows the eigenvalues in a downward curve, from highest to lowest. The first two components can be considered to be the most significant since they contain about 32% of the total information of the data.
#There's a clear drop off after the first two components 
fviz_eig(pca_data, addlabels = TRUE)

#Biplot of attributes - let's just do the first 20 beacuse there's 104 components
#With the biplot, it is possible to visualize the similarities and dissimilarities between the samples, and further shows the impact of each attribute on each of the principal components.
#First, all the variables that are grouped together are positively correlated to each other
#Then, the higher the distance between the variable and the origin, the better represented that variable is
#Finally, variables that are negatively correlated are displayed to the opposite sides of the biplot’s origin. 
fviz_pca_var(pca_data, 
             col.var = "black",
             select.var = list(cos2=20))

#Another way of making the same plot
#fviz_pca_ind(pca_data,
#             col.ind = "cos2", 
#             pointsize = "cos2",
#             gradient.cols = c("#FFCCFF", "#CC0066", "#000000"),
#             repel = TRUE)

#Contribution of each variable
#The goal of this visualization is to determine how much each variable is represented in a given component. Such a quality of representation is called the Cos2 and corresponds to the square cosine, and it is computed using the fviz_cos2 function.
fviz_cos2(pca_data, 
          choice = "var", 
          axes = 1)

#Biplot combined with cos2 
#The biplot and attributes importance can be combined to create a single biplot, where attributes with similar cos2 scores will have similar colors
#From the biplot below:
##High cos2 attributes are colored in green
##Mid cos2 attributes have an orange color
##Finally, low cos2 attributes have a black color 
fviz_pca_var(pca_data, 
             col.var = "cos2",
             gradient.cols = c("black", "orange", "green"),
             repel = TRUE, 
             select.var = list(cos2=20))

#Let's extract the top 20 PCA components for our models
# Extract the results for variables and individuals
top20_pca <- as.data.frame(pca_data$loadings[,1:20])

```

#Alternative PCA calculation - this is what we will use for prelim models
```{r eval = FALSE}

#Prcomp package seems to be better 
#Let's try a different PCA
pca.out <- prcomp(X_train)

#Calculate variance 
pca.var <- pca.out$sdev^2

#Calculate proportion of variance explained 
pve <- pca.var/sum(pca.var)

#Scree plot 
plot(pve, 
     xlab="Principal Component",
     ylab="Proportion of Variance Explained", 
     main="Scree Plot",
     #ylim=c(0, 1),
     type="b")

#Create scree plot using ggplot2 (nicer looking)
pve_df <- data.frame(Principal_Component = 1:length(pve), Variance_Explained = pve)
ggplot(pve_df, aes(x = Principal_Component, y = Variance_Explained)) +
  geom_point() +
  geom_line() +
  labs(title = "Scree Plot", x = "Principal Component", y = "Proportion of Variance Explained") +
  theme_minimal()


#Compute cumulative proportion of variance explained
cum_pve <- cumsum(pve)

#Create cumulative scree plot
plot(cumsum(pve),
     xlab="Principal Component",
     ylab="Cumulative Proportion of Variance Explained",
     main="Cumulative Scree Plot",
     ylim=c(0,1),
     type="b")

#Create cumulative scree plot using ggplot2 (nicer looking)
pve_df$cum_pve <- cum_pve
ggplot(pve_df, aes(x = Principal_Component, y = cum_pve)) +
  geom_point() +
  geom_line() +
  labs(title = "Cumulative Scree Plot", x = "Principal Component", y = "Cumulative Proportion of Variance Explained") +
  theme_minimal()

#Create raw biplot - shows scores and loadings of PCs
biplot(pca.out, scale = 0)

#Extract scores and loadings
scores <- as.data.frame(pca.out$x)
loadings <- as.data.frame(pca.out$rotation)

#Create a data frame for scores with principal component columns
scores_df <- data.frame(scores, Sample = rownames(scores))

#Create a data frame for loadings with variable names
loadings_df <- data.frame(loadings, Variable = rownames(loadings))

#Plot scores (biplot)
ggplot(scores_df, aes(x = PC1, y = PC2, label = Sample)) +
  geom_point() +
  geom_text(vjust = 1.5, hjust = 1.5) +
  labs(title = "PCA Biplot", x = "PC1", y = "PC2") +
  theme_minimal()

#Plot loadings (biplot)
ggplot(loadings_df, aes(x = PC1, y = PC2, label = Variable)) +
  geom_point(color = 'red') +
  geom_text(vjust = 1.5, hjust = 1.5, color = 'red') +
  labs(title = "PCA Biplot", x = "PC1", y = "PC2") +
  theme_minimal()

#Pairs plot - a pairs plot shows scatter plots of the principal component scores against each other
pairs(pca.out$x[, 1:5], main = "Pairs Plot of First 5 Principal Components")

#Construct new data with the PCA selected components
pca_lr_data <- data.frame(Y_train, pca.out$x[,1:20])
head(pca_lr_data)

```


#Model 5: PCA + logistic regression
```{r}

#To get the scores, we need to matrix multiply the loadings with the data 
pca_matrix <- data.matrix(top20_pca)
data_train_matrix <- data.matrix(X_train)
pca_x_train <- data.frame(data_train_matrix %*% pca_matrix)

data_test_matrix <- data.matrix(X_test)
pca_x_test <- data.frame(data_test_matrix%*% pca_matrix)

data_y_test_lr <- data.matrix(Y_test) 
data_y_train_lr <- data.matrix(Y_train)

pca_lr_model <- glm(data_y_train_lr ~ ., 
                    data=pca_x_train, 
                    family="binomial")


summary(pca_lr_model)

#Confusion matrix for training data
predicted_values <- as.data.frame(pca_lr_model$fitted.values)
predicted_classes <- ifelse(predicted_values > 0.5, 1, 0)
colnames(predicted_classes) <- "Predicted"

training_preds_actual <- cbind(data_y_train_lr, predicted_classes)
colnames(training_preds_actual) <- c("Actual", "Predicted")
training_preds_actual <- as.data.frame(training_preds_actual)

Actual <- factor(c(0, 0, 1, 1))
Predicted <- factor(c(0, 1, 0, 1))
values <- table(training_preds_actual)[1:4]
cf_data <- data.frame(Actual, Predicted, values)

ggplot(cf_data =  training_preds_actual, mapping = aes(x = Actual, y = Predicted)) +
  geom_tile(aes(fill = values), colour = "white") +
  geom_text(aes(label = sprintf("%1.0f", values)), vjust = 1) +
  scale_fill_gradient(low = "blue", high = "red") +
  theme_bw() + theme(legend.position = "none") + 
  ggtitle("Training Data PCA + LR: Confusion Matrix")

confusion_matrix <- as.data.frame(table(training_preds_actual))

#Confusion matrix for test data
#Let's get the test set predictions
pca_lr_test_preds <- predict(pca_lr_model, newdata=pca_x_test, type="response")
predicted_classes <- ifelse(pca_lr_test_preds > 0.5, 1, 0)
predicted_classes <- as.data.frame(predicted_classes)
colnames(predicted_classes) <- "Predicted"

test_preds_actual <- cbind(data_y_test_lr, predicted_classes)
colnames(test_preds_actual) <- c("Actual", "Predicted")

Actual <- factor(c(0, 0, 1, 1))
Predicted <- factor(c(0, 1, 0, 1))
values <- table(test_preds_actual)[1:4]
cf_data <- data.frame(Actual, Predicted, values)

ggplot(cf_data =  test_preds_actual, mapping = aes(x = Actual, y = Predicted)) +
  geom_tile(aes(fill = values), colour = "white") +
  geom_text(aes(label = sprintf("%1.0f", values)), vjust = 1) +
  scale_fill_gradient(low = "blue", high = "red") +
  theme_bw() + theme(legend.position = "none") + 
  ggtitle("Test Data PCA + LR: Confusion Matrix")

confusion_matrix <- as.data.frame(table(test_preds_actual))

#ROC curve
pca_lr_roc <- roc(data_y_test_lr, 
                  pca_lr_test_preds,
            #as.numeric(unlist(predicted_classes)), 
            auc=TRUE, 
            ci=TRUE, 
            ci.type="bars",
            plot=TRUE, 
            grid=TRUE, 
            percent=TRUE, 
            boot.n=100, 
            ci.alpha=0.9, 
            stratified=FALSE,
            show.thres=TRUE, 
            legacy.axes=TRUE,
            reuse.auc=TRUE, 
            print.auc=TRUE,
            print.thres.col="blue",
            print.thres.cex=0.7,
            main="Test Data PCA + LR: ROC Curve")

pca_lr_roc

```

#Model 6: PCA + SVM 
```{r}
#pca_x_train is our predictors dataframe
#data_y_train is our label dataframe

#Turn y into a factor variable
data_y_train_svm <- as.factor(Y_train$label)
data_y_test_svm <- as.factor(Y_test$label)

#Create a merged dataframe of predictors and labels
data_train_pca_svm <- data.frame(pca_x_train, y = data_y_train_svm)

#Make a call to svm, using y as the response variable and other variables as the predictors. The dataframe will have unpacked the matrix x into 2 columns named x1 and x2. Kernel is linear, the tune-in parameter cost is 100, and scale equals false. We don't need to scale since that's already been done.

#Tried cost function with 10 and 100, performed worse with cost function 100
pca_svm <- svm(y ~ ., 
              data = data_train_pca_svm, 
              kernel = "linear", 
              cost = 10, 
              scale = FALSE, 
              probability = TRUE)

print(pca_svm)

#There's a plot function for SVM that shows the decision boundary.
#In the plot, points that are represented by an “X” are the support vectors, or the points that directly affect the classification line. The points marked with an “o” are the other points, which don’t affect the calculation of the line. 
#Principal component 1 on x axis and component 2 on y axis
plot(pca_svm, data_train_pca_svm, Comp.2 ~ Comp.1)

#Get our training set predictions for confusion matrix and ROC curve
pca_svm_test_preds <- predict(pca_svm, newdata = pca_x_test, probability=TRUE)
pca_svm_test_preds <- as.data.frame(attr(pca_svm_test_preds, "probabilities"))
colnames(pca_svm_test_preds) <- c("Lose", "Win")
pca_svm_test_preds_win <- pca_svm_test_preds$Win

#Test Data ROC curve
pca_svm_roc <- roc(as.numeric(data_y_test_svm),
                   pca_svm_test_preds_win,
                   auc=TRUE, 
                   ci=TRUE, 
                   ci.type="bars",
                   plot=TRUE, 
                   grid=TRUE, 
                   percent=TRUE, 
                   boot.n=100, 
                   ci.alpha=0.9, 
                   stratified=FALSE,
                   show.thres=TRUE, 
                   legacy.axes=TRUE,
                   reuse.auc=TRUE, 
                   print.auc=TRUE,
                   print.thres.col="blue",
                   print.thres.cex=0.7,
                   main="Test Data PCA + Linear SVM: ROC Curve")

#This model didn't work so well. Everything got predicted to one class. I think the cost function needs to be tuned, although that still doesn't guarantee a better result.


#Trying out a radial kernel with a cost function of 10
pca_svm_radial <- svm(factor(y) ~ ., 
              data = data_train_pca_svm, 
              kernel = "radial", 
              cost = 10, 
              scale = FALSE,
              probability=TRUE)

print(pca_svm_radial)

table(predicted=pca_svm_radial$fitted,actual=data_train_pca_svm$y)

plot(pca_svm_radial, data_train_pca_svm, Comp.2 ~ Comp.1)

#Thousands of support vectors...so basically every point is near the decision boundary ... 

#There's a plot function for SVM that shows the decision boundary.
#In the plot, points that are represented by an “X” are the support vectors, or the points that directly affect the classification line. The points marked with an “o” are the other points, which don’t affect the calculation of the line. 
#Principal component 1 on x axis and component 2 on y axis

#Get our test set predictions for confusion matrix and ROC curve
pca_svm_radial_test_preds <- predict(pca_svm_radial, pca_x_test, probability=TRUE)
pca_svm_radial_test_preds <- as.data.frame(attr(pca_svm_radial_test_preds, "probabilities"))
colnames(pca_svm_radial_test_preds) <- c("Lose", "Win")
pca_svm_radial_test_preds_win <- pca_svm_radial_test_preds$Win


#ROC curve
pca_svm_radial_roc <- roc(data_y_test_svm,
                          pca_svm_radial_test_preds_win,
                   auc=TRUE, 
                   ci=TRUE, 
                   ci.type="bars",
                   plot=TRUE, 
                   grid=TRUE, 
                   percent=TRUE, 
                   boot.n=100, 
                   ci.alpha=0.9, 
                   stratified=FALSE,
                   show.thres=TRUE, 
                   legacy.axes=TRUE,
                   reuse.auc=TRUE, 
                   print.auc=TRUE,
                   print.thres.col="blue",
                   print.thres.cex=0.7,
                   main="Test Set PCA + Radial SVM: ROC Curve")

pca_svm_radial_roc

```

#Model 7: PCA + Random Forest
```{r}

data_y_pca_rf <- as.factor(Y_train$label)

pca_rf <- randomForest(y=data_y_pca_rf, 
                       x=pca_x_train, 
                       proximity=TRUE,
                       replace=TRUE,
                       importance=TRUE) 

print(pca_rf)

#Out of bag error is 29.49%, so the train data set model accuracy is around 70%.
#500 trees, 4 variables at each split

#Let's look at feature importance, i.e. the top 10 most important features
head(pca_rf$importance, 10)

#We can see how it classifies each sample
first <- as.data.frame(pca_rf$votes)
second <- data_y_pca_rf
head(cbind(first, second), 10)
tail(cbind(first, second), 10)

#Visualizations
plot(pca_rf,
     main="PCA Random Forest Error Rate",
     #xlab="Number of Trees",
     #ylab="Error Rate", 
     lwd=1.5,
     panel.first={
        axis(1, tck=1, col.ticks="light gray")
        axis(1, tck=-0.015, col.ticks="black")
        axis(2, tck=1, col.ticks="light gray", lwd.ticks="1")
        axis(2, tck=-0.015)
        minor.tick(nx=5, ny=2, tick.ratio=0.5)
        box()
 })

#Tree size
hist(treesize(pca_rf),
     main = "No. of Nodes for the Trees",
     col = "green", 
     xlab="Tree Size",
     breaks=20)

#Variable Importance
varImpPlot(pca_rf,
           sort = T,
           n.var = 10,
           main = "Top 10 - Variable Importance")

#Get predictions
data_y_test_pca_rf <- as.factor(Y_test$label)
pca_rf_test_preds <- predict(pca_rf, newdata = pca_x_test, type="prob")
pca_rf_test_preds <- as.data.frame(pca_rf_test_preds)
colnames(pca_rf_test_preds) <- c("Lose", "Win")
pca_rf_test_preds_win <- pca_rf_test_preds$Win

#Test set ROC curve
pca_rf_roc <- roc(data_y_test_pca_rf, 
            pca_rf_test_preds_win, 
            auc=TRUE, 
            ci=TRUE, 
            ci.type="bars",
            plot=TRUE, 
            grid=TRUE, 
            percent=TRUE, 
            boot.n=100, 
            ci.alpha=0.9, 
            stratified=FALSE,
            show.thres=TRUE, 
            legacy.axes=TRUE,
            reuse.auc=TRUE, 
            print.auc=TRUE,
            print.thres.col="blue",
            print.thres.cex=0.7,
            main="PCA + RF: ROC Curve")

```

#Model 8: PCA + XGBoost
```{r}
data_train_xgboost <- data_train_pca_svm

pca_xgboost <- xgboost(data = data.matrix(data_train_xgboost[, -21]), 
                       label = (as.numeric(data_train_xgboost[,21]) -1), 
                       eta = 0.1, 
                       nrounds = 25, 
                       #max_depth = 15,
                       #subsample = 0.5,
                       objective = "binary:logistic")


#Let's predict
pca_xgb_test_pred <- predict(pca_xgboost, data.matrix(pca_x_test))
print(head(pca_xgb_test_pred))

#Convert to factor 
pca_xgb_test_pred_class <- as.numeric(pca_xgb_test_pred > 0.5)
print(head(pca_xgb_test_pred_class))

#Measure model performance
err <- mean(as.numeric(pca_xgb_test_pred_class > 0.5) != Y_test$label)
print(paste("test-error=", err))

#Confusion matrix 
predicted_classes <- as.data.frame(pca_xgb_test_pred_class)
colnames(predicted_classes) <- "Predicted"

test_preds_actual <- cbind(Y_test, predicted_classes)
colnames(test_preds_actual) <- c("Actual", "Predicted")

Actual <- factor(c(0, 0, 1, 1))
Predicted <- factor(c(0, 1, 0, 1))
values <- table(test_preds_actual)[1:4]
cf_data <- data.frame(Actual, Predicted, values)

ggplot(cf_data=test_preds_actual, mapping = aes(x = Actual, y = Predicted)) +
  geom_tile(aes(fill = values), colour = "white") +
  geom_text(aes(label = sprintf("%1.0f", values)), vjust = 1) +
  scale_fill_gradient(low = "blue", high = "red") +
  theme_bw() + theme(legend.position = "none") + 
  ggtitle("Test Set RF + LR: Confusion Matrix")

confusion_matrix <- as.data.frame(table(training_preds_actual))

#Test Set ROC curve
pca_xgb_roc <- roc(as.numeric(Y_test$label),
                  as.numeric(pca_xgb_test_pred),
                   auc=TRUE, 
                   ci=TRUE, 
                   ci.type="bars",
                   plot=TRUE, 
                   grid=TRUE, 
                   percent=TRUE, 
                   boot.n=100, 
                   ci.alpha=0.9, 
                   stratified=FALSE,
                   show.thres=TRUE, 
                   legacy.axes=TRUE,
                   reuse.auc=TRUE, 
                   print.auc=TRUE,
                   print.thres.col="blue",
                   print.thres.cex=0.7,
                   main="Test Set RF + XGBoost: ROC Curve")

pca_xgb_roc

```

#Model 9: Shallow neural net
```{r}
#https://www.geeksforgeeks.org/build-a-neural-network-classifier-in-r/#

#We already have our split data
#X_train_rf
#X_test_rf
#Y_train
#Y_test

# Prepare the data for training
train_features <- as.matrix(X_train_rf)
train_labels <- to_categorical(Y_train$label, num_classes = 2)
test_features <- as.matrix(X_test_rf)
test_labels <- to_categorical(Y_test$label, num_classes = 2)

set.seed(42)
#Compile the model 
model <- keras_model_sequential() %>%
  layer_dense(units = 10, activation = 'relu', input_shape = c(20)) %>%
  #layer_dense(units = 5, activation = 'relu', input_shape = c(10)) %>% #adding an extra layer didn't help 
  layer_dense(units = 2, activation = 'sigmoid')

# Compile the model
model %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_adam(),
  metrics = c('accuracy', metric_auc())
)
 
# Print the model summary
summary(model)

#Train the model
history <- model %>% fit(
  x = train_features,
  y = train_labels,
  epochs = 50,
  batch_size = 32,
  validation_split = 0.2
)

history

eval_result <- model %>% evaluate(
  x = test_features,
  y = test_labels
)
 
cat("Test loss:", eval_result[1], "\n")
cat("Test accuracy:", eval_result[2], "\n")

#Not really a great performance 
```


#Code references
```{r}
#https://www.kaggle.com/code/sadz2201/tennis-exploration-atpboost
#https://www.kaggle.com/code/jedipro/atp-men-s-tour-predictive-model-with-xgboost/script
#https://github.com/BrandoPolistirolo/Tennis-Betting-ML/blob/main/Training.py
#https://github.com/polmarin/Data-Analysis-Tennis/blob/main/Portfolio%20Project.ipynb <-- THIS IS THE WAY
#https://github.com/VincentAuriau/Tennis-Prediction/blob/master/python/evaluation/train_test.py
#https://github.com/jugalm/predicting-wimbledon-matches/blob/master/nn_model.ipynb
#https://github.com/MareoRaft/tennis/tree/master/frontend
#https://github.com/shukkkur/Tennis-Match-Prediction/blob/main/ModelsAndPredictions.ipynb
#https://github.com/chief-r0cka/MLT/blob/master/Eda-data-prep-modeling-predictions-then-betting.ipynb
#https://github.com/andmon97/ATPTennisMatchPredictions/blob/main/ML_tennis.pdf
#https://github.com/edouardthom/ATPBetting/blob/master/Beating%20the%20bookmakers%20on%20tennis%20matches.ipynb
#https://github.com/molson194/Tennis-Betting-ML/blob/master/train.py
#https://github.com/rajdua22/tennis_betting
#https://github.com/chief-r0cka/MLT/blob/master/Eda-data-prep-modeling-predictions-then-betting.ipynb 
#https://hub.docker.com/r/mcekovic/uts-database
#https://github.com/mcekovic/tennis-crystal-ball/issues 
#https://github.com/johncookds/Predicting-Tennis-Matches/blob/master/MLFinalpaper.Rmd
#https://github.com/polmarin/Data-Analysis-Tennis/blob/main/Portfolio%20Project.ipynb

```
