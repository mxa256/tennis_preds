---
title: "Unseen_Test_Set"
output: html_document
date: "2024-04-10"
---
#Packages
```{r}
library(dplyr)
library(tidyverse)
library(ggplot2)
library(caret)
library(MLmetrics)
library(e1071)
library(pROC)
library(xgboost)

```

#Functions
```{r}

get_predictions_on_test <- function(y_test, x_test, predictions, confidence, clean_df) {
    
    #y_test and x_test make up the test set - labels and text respectively
    #predictions are the predicted labels
    #confidence is the confidence score (probability)
    #clean_df is used to get the Label Studio IDs and join it to the test set by joining on text field
    
  print(paste("confidence", confidence))
  
  y_test_model <- data.frame(y_test)
  cat("test df:\n")
  print(head(y_test_model, 5))
  
  x_test_model <- data.frame(x_test)
  cat("x_test:\n")
  print(head(x_test_model, n = 5))
  
  #Add observations to y_test
  y_test_model <- cbind(y_test_model, x_test_model)
  cat("test df after variables:\n")
  print(head(y_test_model, n = 5))
  
  # Create prediction data frame
  cat("make the prediction df\n")
  print(head(y_test_model, n = 5))
  
  # Join predictions to test set
  y_test_model$prediction <- predictions
  cat("add the confidence to the df\n")
  print(head(y_test_model, n = 5))
  pred_dist <- table(y_test_model$prediction)
  cat("prediction distribution\n", pred_dist, "\n")
  
  # Add confidence score
  y_test_model$probability <- round(confidence, digits = 4)
  
  clean_df_copy <- clean_df
  clean_df_copy$index <- seq_len(nrow(clean_df_copy)) # Add index column
  cat("orig df\n")
  print(head(clean_df_copy, n = 5))
  
  # Merge with clean_df
  pred_df2 <- merge(y_test_model, clean_df_copy, all.x = TRUE)
  cat("pred df")
  print(head(pred_df2, n = 5))
  
  # Return prediction data frame and prediction distribution
  return(list(pred_df2, as.vector(pred_dist)))
  
}

#Generate calibration curve 
calibration_curve <- function(y_true, y_pred, n_bins = 20) {
  bin_size <- length(y_pred) / n_bins
  thresholds <- seq(0, 1, length.out = n_bins + 1)
  mean_predicted_value <- numeric(n_bins)
  fraction_of_positives <- numeric(n_bins)
  
  for (i in 1:n_bins) {
    bin_start <- (i - 1) * bin_size + 1
    bin_end <- min(i * bin_size, length(y_pred))
    bin_indices <- bin_start:bin_end
    mean_predicted_value[i] <- mean(y_pred[bin_indices])
    fraction_of_positives[i] <- sum(y_true[bin_indices] == 1) / length(bin_indices)
  }
  
  return(list(mean_predicted_value = mean_predicted_value, fraction_of_positives = fraction_of_positives))
}

#Calculate Brier score loss
brier_score_loss <- function(y_true, y_pred) {
  mean((y_true - y_pred)^2)
}
```

#Unseen test set 
```{r}
#Our test set will be 2023 matches thus far 
#Read in the data
data_ids <- read.csv("/Users/mona/Dropbox/Desktop/Tennis_Analytics/tennis_preds/data/test2023_data_ids.csv")
data_train <- read.csv("/Users/mona/Dropbox/Desktop/Tennis_Analytics/tennis_preds/data/test2023_data_train.csv")

```

#Process for modeling
```{r}
#Make copies
data_ids_copy <- data_ids
data_train_copy <- data_train

#Scale the data 
data_train_copy <- data_train_copy %>% select(!row_id)
clean_data_frame <- as.data.frame(scale(data_train_copy))

#Rename one variable
clean_data_frame <- clean_data_frame %>% rename(rank_diff = rank_diff_P_1)

#Set the label 
y_test <- as.data.frame(data_ids_copy$Win_P_1)
colnames(y_test) <- "label"

```

#Model Testing
```{r}
#Model 1: RF + SVM Radial Kernel
model_path <- "/Users/mona/Dropbox/Desktop/Tennis_Analytics/tennis_preds/mlruns/545417160078729357/48b2820e7fb6429d92240000e45ad961/artifacts/model.rds"

loaded_model <- readRDS(file = model_path)

#Get predictions
predictions <- predict(loaded_model,
                       clean_data_frame,
                       probability=TRUE)
           
#Extract probabilities for Win
test_pred_probs <- as.data.frame(attr(predictions, "probabilities"))[2]
           
#Confusion matrix
test_pred_classes <- ifelse(test_pred_probs > 0.5, 1, 0)
cm <- caret::confusionMatrix(factor(test_pred_classes), 
                             factor(y_test$label),
                             mode="everything", 
                             positive = "1")
           
cm_0 <- caret::confusionMatrix(factor(test_pred_classes),
                               factor(y_test$label),
                               mode="everything", 
                               positive = "0")
           
#Plot confusion matrix
group_names <- c('TrueNeg', 'FalsePos', 'FalseNeg', 'TruePos')
group_counts <- c(sprintf('%0.0f', as.vector(cm$table)))
group_percentages <- sprintf('%0.2f%%', as.vector(cm$table / sum(cm$table)) * 100)
colnames(test_pred_classes) <- "Predicted"
preds_actual <- cbind(y_test, test_pred_classes)
colnames(preds_actual) <- c("Actual", "Predicted")
Actual <- factor(c(0, 0, 1, 1))
Predicted <- factor(c(0, 1, 0, 1))
values <- table(preds_actual)[1:4]
cf_data <- data.frame(Actual, Predicted, values)
           
cf_matrix <- ggplot(cf_data=preds_actual, mapping = aes(x = Actual, y = Predicted)) +
  geom_tile(aes(fill = values), colour = "white") +
  geom_text(aes(label = group_counts), vjust = 2.0) +
  geom_text(aes(label = group_percentages), vjust = 4.0) +
  geom_text(aes(label = group_names), vjust = 0) +
  scale_fill_gradient(low = "blue", high = "red") +
  theme_bw() + theme(legend.position = "none") + 
  ggtitle("Confusion Matrix")

cf_matrix

#Calibration Curve 
#brierScore <- BrierScore(y_test, predictions, scaled=TRUE)
y_test_num <- as.numeric(unlist(y_test))
y_probs_num <- as.numeric(unlist(test_pred_probs))
calibration_data <- calibration_curve(y_test_num, y_probs_num, n_bins=20)
calibration_df <- data.frame(mean_predicted_value = calibration_data$mean_predicted_value,
                             fraction_of_positives = calibration_data$fraction_of_positives)
           
clf_score <- brier_score_loss(y_test_num, test_pred_classes)
           
ggplot(calibration_df, aes(x = mean_predicted_value, y = fraction_of_positives)) +
  geom_point(color="red") + 
  geom_line(color="red") + 
  labs(title = paste("Calibration Curve for Positive Class (Brier loss =", round(clf_score, 2), ")"),
  x = "Mean predicted probability", 
  y = "Fraction of positives") + 
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") + 
  xlim(0, 1) +
  ylim(0, 1) +
  theme_minimal() 
           
#Get the dataframe for review 
#Get the distribution of predictions for assurance and documentation
pred_result <- get_predictions_on_test(y_test, clean_data_frame, test_pred_classes, y_probs_num, clean_data_frame)
pred_df <- pred_result[[1]][21:24]
pred_distribution <- pred_result[[2]]
      
# Initialize an empty list to store metrics
metrics <- list()
metrics$test_accuracy <- MLmetrics::Accuracy(test_pred_classes, y_test)
           
metrics$tn <- cm[[2]][1]
metrics$fn <- cm[[2]][3]
metrics$fp <- cm[[2]][2]
metrics$tp <- cm[[2]][4]
           
metrics$f1_score <- cm$byClass['F1']

metrics$recall_0 <- cm_0$byClass['Recall']
metrics$recall_1 <- cm$byClass['Recall']
metrics$precision_0 <- cm_0$byClass['Precision']
metrics$precision_1 <- cm$byClass['Precision']

#ROC curve on the test set 
roc <- pROC::roc(y_test_num,
                 y_probs_num,
                 auc=TRUE, 
                 ci=TRUE, 
                 ci.type="bars",
                 plot=TRUE, 
                 grid=TRUE, 
                 percent=TRUE, 
                 boot.n=100, 
                 ci.alpha=0.9, 
                 stratified=FALSE,
                 show.thres=TRUE, 
                 legacy.axes=TRUE,
                 reuse.auc=TRUE, 
                 print.auc=TRUE,
                 print.thres.col="blue",
                 print.thres.cex=0.7,
                 main="2023 Test Set ROC")
           
#Save the metric
metrics$auc <- roc$auc[1]
          
for (metric in metrics) {
  print(metric)
}

```

#Model Testing
```{r}

#Model 2: XGBoost Top Performer
model_path <- "/Users/mona/Dropbox/Desktop/Tennis_Analytics/tennis_preds/mlruns/488956178776541576/69fbce8b1ec1446e84e89fe64101fbf0/artifacts/model.rds"

#Model 3: XGBoost Optimal Performer 
model_path <- "/Users/mona/Dropbox/Desktop/Tennis_Analytics/tennis_preds/mlruns/488956178776541576/cf90b13f1e63481f975b738e9b37a256/artifacts/model.rds"

loaded_model <- readRDS(file = model_path)

#For XGBoost, need to change data type
clean_data_matrix <- data.matrix(clean_data_frame)

#Get predictions
predictions <- predict(loaded_model,
                       clean_data_matrix,
                       probability=TRUE)
           
#Extract probabilities for Win
test_pred_probs <- predictions
           
#Confusion matrix
test_pred_classes <- ifelse(test_pred_probs > 0.5, 1, 0)
cm <- caret::confusionMatrix(factor(test_pred_classes), 
                             factor(y_test$label),
                             mode="everything", 
                             positive = "1")
           
cm_0 <- caret::confusionMatrix(factor(test_pred_classes),
                               factor(y_test$label),
                               mode="everything", 
                               positive = "0")
           
#Plot confusion matrix
group_names <- c('TrueNeg', 'FalsePos', 'FalseNeg', 'TruePos')
group_counts <- c(sprintf('%0.0f', as.vector(cm$table)))
group_percentages <- sprintf('%0.2f%%', as.vector(cm$table / sum(cm$table)) * 100)
#colnames(test_pred_classes) <- "Predicted"
preds_actual <- cbind(y_test, test_pred_classes)
colnames(preds_actual) <- c("Actual", "Predicted")
Actual <- factor(c(0, 0, 1, 1))
Predicted <- factor(c(0, 1, 0, 1))
values <- table(preds_actual)[1:4]
cf_data <- data.frame(Actual, Predicted, values)
           
cf_matrix <- ggplot(cf_data=preds_actual, mapping = aes(x = Actual, y = Predicted)) +
  geom_tile(aes(fill = values), colour = "white") +
  geom_text(aes(label = group_counts), vjust = 2.0) +
  geom_text(aes(label = group_percentages), vjust = 4.0) +
  geom_text(aes(label = group_names), vjust = 0) +
  scale_fill_gradient(low = "blue", high = "red") +
  theme_bw() + theme(legend.position = "none") + 
  ggtitle("Confusion Matrix")

cf_matrix

#Calibration Curve 
#brierScore <- BrierScore(y_test, predictions, scaled=TRUE)
y_test_num <- as.numeric(unlist(y_test))
y_probs_num <- as.numeric(unlist(test_pred_probs))
calibration_data <- calibration_curve(y_test_num, y_probs_num, n_bins=20)
calibration_df <- data.frame(mean_predicted_value = calibration_data$mean_predicted_value,
                             fraction_of_positives = calibration_data$fraction_of_positives)
           
clf_score <- brier_score_loss(y_test_num, test_pred_classes)
           
ggplot(calibration_df, aes(x = mean_predicted_value, y = fraction_of_positives)) +
  geom_point(color="red") + 
  geom_line(color="red") + 
  labs(title = paste("Calibration Curve for Positive Class (Brier loss =", round(clf_score, 2), ")"),
  x = "Mean predicted probability", 
  y = "Fraction of positives") + 
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") + 
  xlim(0, 1) +
  ylim(0, 1) +
  theme_minimal() 
           
#Get the dataframe for review 
#Get the distribution of predictions for assurance and documentation
pred_result <- get_predictions_on_test(y_test, clean_data_frame, test_pred_classes, y_probs_num, clean_data_frame)
pred_df <- pred_result[[1]][21:24]
pred_distribution <- pred_result[[2]]
      
# Initialize an empty list to store metrics
metrics <- list()
metrics$test_accuracy <- MLmetrics::Accuracy(test_pred_classes, y_test)
           
metrics$tn <- cm[[2]][1]
metrics$fn <- cm[[2]][3]
metrics$fp <- cm[[2]][2]
metrics$tp <- cm[[2]][4]
           
metrics$f1_score <- cm$byClass['F1']

metrics$recall_0 <- cm_0$byClass['Recall']
metrics$recall_1 <- cm$byClass['Recall']
metrics$precision_0 <- cm_0$byClass['Precision']
metrics$precision_1 <- cm$byClass['Precision']

#ROC curve on the test set 
roc <- pROC::roc(y_test_num,
                 y_probs_num,
                 auc=TRUE, 
                 ci=TRUE, 
                 ci.type="bars",
                 plot=TRUE, 
                 grid=TRUE, 
                 percent=TRUE, 
                 boot.n=100, 
                 ci.alpha=0.9, 
                 stratified=FALSE,
                 show.thres=TRUE, 
                 legacy.axes=TRUE,
                 reuse.auc=TRUE, 
                 print.auc=TRUE,
                 print.thres.col="blue",
                 print.thres.cex=0.7,
                 main="2023 Test Set ROC")
           
#Save the metric
metrics$auc <- roc$auc[1]
          
for (metric in metrics) {
  print(metric)
}

```


